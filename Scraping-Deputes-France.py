#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Scraping-Deputes-France.py

Script pour scraper les d√©put√©¬∑e¬∑s fran√ßais (Nom, R√©gion, Email, Groupe, Circonscription)
depuis le site de l'Assembl√©e nationale.

- Gestion des retries, d√©lais, timeout, et multithreading
- Option d'affichage sous forme de tableau ASCII

Utilisation :
  python3 scrape_deputes_france.py --help  # Afficher l'aide compl√®te
"""


import argparse
import concurrent.futures
import re
import time
from typing import Dict, List, Optional

import requests
from bs4 import BeautifulSoup


BASE_URL: str = "https://www.assemblee-nationale.fr"
DEPUTES_URL: str = "https://www2.assemblee-nationale.fr/deputes/liste/regions"


# Liste des r√©gions (structur√©es en <h2> sur la page) valides sur le site de l'Assembl√©e nationale
VALID_REGIONS: List[str] = [
    "Auvergne-Rh√¥ne-Alpes",
    "Bourgogne-Franche-Comt√©",
    "Bretagne",
    "Centre-Val de Loire",
    "Corse",
    "Grand Est",
    "Hauts-de-France",
    "Ile-de-France",
    "Normandie",
    "Nouvelle-Aquitaine",
    "Occitanie",
    "Pays de la Loire",
    "Provence-Alpes-C√¥te d'Azur",
    "R√©union"
]


def normalize_region(region: str) -> Optional[str]:
    """
    Normalise un nom de r√©gion : supprime les espaces en trop et met en correspondance
    avec une r√©gion valide en ignorant la casse.

    Args:
        region (str): Nom de la r√©gion entr√©e par l'utilisateur.

    Returns:
        str | None: Nom normalis√© de la r√©gion si valide, sinon None.
    """
    region = region.strip().lower()
    for valid_region in VALID_REGIONS:
        if region == valid_region.lower():
            return valid_region
    return None


def get_with_retries(
    url: str,
    max_retries: int,
    delay_between: float,
    timeout: float,
    debug: bool
) -> Optional[requests.Response]:
    """
    Effectue plusieurs tentatives d'une requ√™te GET sur une URL donn√©e.

    Attends `delay_between` secondes entre chaque tentative si la pr√©c√©dente a √©chou√©.
    Le timeout de la requ√™te est fix√© par `timeout`.

    Args:
        url (str): L'URL cible.
        max_retries (int): Nombre maximal de tentatives.
        delay_between (float): D√©lai entre les tentatives en secondes.
        timeout (float): Dur√©e maximale d'attente pour la requ√™te.
        debug (bool): Active le mode debug.

    Returns:
        Optional[requests.Response]: R√©ponse HTTP si succ√®s, sinon None.
    """
    for attempt in range(1, max_retries + 1):
        try:
            if debug:
                print(f"[DEBUG] Attempt {attempt}/{max_retries} fetching: {url}")
            resp = requests.get(url, timeout=timeout)
            resp.raise_for_status()
            return resp
        except requests.RequestException as exc:
            print(f"[ERROR] Attempt {attempt} failed for {url}: {exc}")
            if attempt < max_retries and delay_between > 0:
                if debug:
                    print(f"[DEBUG] Sleeping {delay_between}s before retrying...")
                time.sleep(delay_between)
    return None


def get_deputes_from_region(
    region_name: str,
    max_retries: int,
    delay_between: float,
    timeout: float,
    debug: bool = False
) -> Dict[str, str]:
    """
    R√©cup√®re une liste de d√©put√©¬∑e¬∑s pour une r√©gion donn√©e.

    Cette fonction extrait les noms et URLs des d√©put√©¬∑e¬∑s d'une r√©gion sp√©cifique
    sur la page `DEPUTES_URL`. L'HTML est structur√© en sections `<h2>` pour les
    r√©gions, `<h4 class='departementTitre'>` pour les d√©partements, et des `<li>`
    contenant les liens vers les fiches des d√©put√©s.

    Args:
        region_name (str): Nom de la r√©gion √† scraper.
        max_retries (int): Nombre maximal de tentatives pour r√©cup√©rer la page.
        delay_between (float): Temps d'attente entre chaque tentative (secondes).
        timeout (float): Temps limite d'attente pour la requ√™te (secondes).
        debug (bool, optional): Active le mode debug. Par d√©faut `False`.

    Returns:
        Dict[str, str]: Dictionnaire `{Nom d√©put√©: URL}` des d√©put√©¬∑e¬∑s trouv√©s.
    """
    if debug:
        print(f"[DEBUG] Collecting deputies for region: {region_name}")

    resp = get_with_retries(
        DEPUTES_URL,
        max_retries=max_retries,
        delay_between=delay_between,
        timeout=timeout,
        debug=debug
    )
    if not resp:
        print(f"[ERROR] Could not fetch region page: {DEPUTES_URL}")
        return {}

    soup = BeautifulSoup(resp.text, "html.parser")
    region_h2 = None

    # Trouver la balise <h2> correspondant √† la r√©gion recherch√©e (region_name)
    for h2_tag in soup.find_all("h2"):
        if h2_tag.get_text(strip=True) == region_name:
            region_h2 = h2_tag
            break

    if not region_h2:
        if debug:
            print(f"[WARNING] No <h2> found for region {region_name}.")
        return {}

    deputes_map: Dict[str, str] = {}

    # Parcourir les √©l√©ments suivants dans l'HTML
    for sibling in region_h2.next_siblings:
        if sibling.name == "h2":
            # Nouvelle r√©gion d√©tect√©e -> arr√™t
            break
        if sibling.name == "h4" and sibling.get("class") == ["departementTitre"]:
            # R√©cup√©rer les <li> suivants contenant les d√©put√©s
            for sub_sib in sibling.next_siblings:
                if sub_sib.name in ("h4", "h2"):
                    # Nouvelle r√©gion ou d√©partement -> arr√™t
                    break
                if sub_sib.name == "div":
                    li_tags = sub_sib.find_all("li")
                    for li_tag in li_tags:
                        a_tag = li_tag.find("a", href=True)
                        if a_tag and a_tag["href"].startswith("/deputes/fiche/"):
                            name = a_tag.get_text(strip=True)
                            full_url = BASE_URL + a_tag["href"]
                            deputes_map[name] = full_url

    if debug:
        print(f"[DEBUG] Deputies found for {region_name}: {list(deputes_map.keys())}")
    return deputes_map


def get_depute_info(
    name: str,
    url: str,
    region: str,
    max_retries: int,
    delay_between: float,
    timeout: float,
    debug: bool = False
) -> Dict[str, Optional[str]]:
    """
    R√©cup√®re les informations d√©taill√©es d'un d√©put√©.

    Cette fonction extrait les informations suivantes depuis la page du d√©put√© :
    - Nom
    - R√©gion
    - Email (extrait du lien `mailto:`)
    - Groupe parlementaire
    - Circonscription

    Elle transforme l'URL `/deputes/fiche/OMC_PAxxxxxx` en `/dyn/deputes/PAxxxxxx`
    pour acc√©der √† la version dynamique du profil.

    Args:
        name (str): Nom du d√©put√©.
        url (str): URL du profil du d√©put√© sur le site de l'Assembl√©e nationale.
        region (str): R√©gion d'√©lection du d√©put√©.
        max_retries (int): Nombre maximal de tentatives en cas d'√©chec.
        delay_between (float): Temps d'attente entre les tentatives (secondes).
        timeout (float): D√©lai maximal d'attente pour la requ√™te (secondes).
        debug (bool, optional): Active le mode debug. Par d√©faut `False`.

    Returns:
        Dict[str, Optional[str]]: Dictionnaire contenant :
            - "nom" (str)
            - "region" (str)
            - "email" (str ou None)
            - "groupe" (str ou None)
            - "circonscription" (str ou None)
    """
    # Extraire l'ID du d√©put√© (ex: OMC_PA12345)
    match_id = re.search(r"/deputes/fiche/OMC_PA(\d+)", url)
    if not match_id:
        if debug:
            print(f"[WARNING] Can't extract OMC_PA ID from {url}")
        return {
            "nom": name,
            "region": region,
            "email": None,
            "groupe": None,
            "circonscription": None,
        }

    deputy_id = f"PA{match_id.group(1)}"
    dyn_url = f"{BASE_URL}/dyn/deputes/{deputy_id}"

    # R√©cup√©ration de la page dynamique du d√©put√©
    resp = get_with_retries(
        dyn_url, max_retries, delay_between, timeout, debug
    )
    if not resp:
        if debug:
            print(f"[ERROR] Could not fetch {dyn_url} after retries.")
        return {
            "nom": name,
            "region": region,
            "email": None,
            "groupe": None,
            "circonscription": None,
        }

    soup = BeautifulSoup(resp.text, "html.parser")

    # Extraction de l'email (mailto:)
    a_mail = soup.find("a", href=re.compile(r"^mailto:"))
    email = a_mail["href"].replace("mailto:", "") if a_mail else None
    if debug:
        print(f"[DEBUG] Email for {name} => {email}")

    # Extraction du groupe parlementaire
    group_tag = soup.find("a", class_="h4 _colored link")
    group = group_tag.get_text(strip=True) if group_tag else None

    # Extraction de la circonscription
    circ_div = soup.find("div", class_="_mb-small._centered-text")
    # Correction : s'il y a un point dans la classe -> on retente
    if not circ_div:
        circ_div = soup.find("div", class_="_mb-small _centered-text")
    circonscription = None
    if circ_div:
        big_span = circ_div.find("span", class_="_big")
        if big_span:
            circonscription = big_span.get_text(strip=True)

    return {
        "nom": name,
        "region": region,
        "email": email,
        "groupe": group,
        "circonscription": circonscription,
    }


def build_ascii_table(
    results: List[Dict[str, Optional[str]]],
    fields: List[str]
) -> str:
    """
    Construit un tableau ASCII r√©capitulatif des d√©put√©s.

    Cette fonction g√©n√®re un tableau ASCII √† partir des donn√©es des d√©put√©s.
    Elle ajuste dynamiquement la largeur des colonnes pour une meilleure lisibilit√©.

    Args:
        results (List[Dict[str, Optional[str]]]): 
            Liste des informations des d√©put√©s (Nom, R√©gion, Email...).
        fields (List[str]): 
            Liste des colonnes √† afficher.

    Returns:
        str: Repr√©sentation ASCII du tableau.
    """
    if not results or not fields:
        return "[INFO] Aucune donn√©e disponible pour g√©n√©rer un tableau."

    # Cr√©ation de l'en-t√™te
    header = [field.capitalize() for field in fields]
    rows = [header]

    # Ajoute les donn√©es
    for dep in results:
        row = [dep.get(f, "") or "" for f in fields]
        rows.append(row)

    # Largeur max. pour chaque colonne
    col_widths: List[int] = []
    for c in range(len(fields)):
        col_widths.append(
            max(len(str(rows[r][c])) for r in range(len(rows)))
        )

    # Construction du tableau
    lines: List[str] = []
    for i, row in enumerate(rows):
        cells = [str(cell).ljust(col_widths[c]) for c, cell in enumerate(row)]
        line = " | ".join(cells)
        lines.append(line)
        if i == 0:
            sep = "-+-".join("-" * w for w in col_widths)
            lines.append(sep)

    return "\n".join(lines)


def scrape_deputes(
    regions: List[str],
    multithreading: bool = False,
    max_threads: int = 5,
    output_file: Optional[str] = None,
    debug: bool = False,
    retries: int = 3,
    delay: float = 0.0,
    req_timeout: float = 10.0,
    fields: Optional[List[str]] = None,
    use_table: bool = False,
    barefields: bool = False,
    no_separator: bool = False,
) -> None:
    """
    Scrape les informations des d√©put√©¬∑e¬∑s fran√ßais (Nom, R√©gion, Email, Groupe, Circonscription)
    pour les r√©gions sp√©cifi√©es.

    Args:
        regions (List[str]): Liste des r√©gions √† scraper.
        multithreading (bool): Active le mode multithreading pour acc√©l√©rer le scraping.
        max_threads (int): Nombre maximum de threads utilis√©s si multithreading est activ√©.
        output_file (Optional[str]): Nom du fichier o√π enregistrer les r√©sultats (si fourni).
        debug (bool): Active le mode debug pour afficher des logs d√©taill√©s.
        retries (int): Nombre maximum de tentatives en cas d'√©chec des requ√™tes.
        delay (float): D√©lai en secondes entre les tentatives en cas d'√©chec.
        req_timeout (float): Timeout (secondes) des requ√™tes HTTP.
        fields (Optional[List[str]]): Liste des champs √† extraire (nom, email...).
        use_table (bool): G√©n√®re un tableau ASCII r√©capitulatif des d√©put√©s.
        barefields (bool): Affiche uniquement les valeurs sans labels (utile pour export CSV-like).
        no_separator (bool): Supprime la ligne de s√©paration si --barefields + 1 champ.

    Returns:
        None: Affiche les r√©sultats dans la console ou les enregistre dans un fichier.
    """
    if fields is None:
        fields = ["nom", "region", "email", "groupe", "circonscription"]

    # 1) Collecte des URLs des d√©put√©s par r√©gion
    deputes_data: List[tuple] = []
    for region in regions:
        region_map = get_deputes_from_region(
            region,
            max_retries=retries,
            delay_between=delay,
            timeout=req_timeout,
            debug=debug
        )
        for dep_name, dep_url in region_map.items():
            deputes_data.append((dep_name, dep_url, region))

    if debug:
        print(f"[DEBUG] Found {len(deputes_data)} deputies total.")

    # 2) R√©cup√©ration des informations d√©taill√©es
    results: List[Dict[str, Optional[str]]] = []
    if multithreading:
        if debug:
            print(f"[DEBUG] Using multithreading with {max_threads} workers.")
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            future_map = {
                executor.submit(
                    get_depute_info,
                    dep_name,
                    dep_url,
                    dep_region,
                    retries,
                    delay,
                    req_timeout,
                    debug
                ): (dep_name, dep_url, dep_region)
                for (dep_name, dep_url, dep_region) in deputes_data
            }
            for future in concurrent.futures.as_completed(future_map):
                results.append(future.result())
    else:
        if debug:
            print("[DEBUG] Running sequentially.")
        for (dep_name, dep_url, dep_region) in deputes_data:
            info = get_depute_info(
                dep_name, dep_url, dep_region,
                retries, delay, req_timeout, debug
            )
            results.append(info)

    # 3) Mise en forme des r√©sultats
    lines: List[str] = []
    # Condition : 1 seul champ, barefields, no_separator -> pas de lignes de tirets
    skip_separators: bool = (
        barefields and len(fields) == 1 and no_separator
    )

    for dep in results:
        for field in fields:
            val: str = dep.get(field, "") or ""
            if barefields:
                lines.append(val)
            else:
                lines.append(f"{field.capitalize()}: {val}")
        if not skip_separators:
            lines.append("-" * 40)

    # 4) G√©n√©ration du tableau ASCII
    ascii_table: str = ""
    if use_table:
        ascii_table = "\n\n=== TABLEAU R√âCAPITULATIF ===\n"
        ascii_table += build_ascii_table(results, fields)
        ascii_table += "\n"

    final_output: str = "\n".join(lines) + ascii_table

    # 5) Enregistrement ou affichage
    if output_file:
        with open(output_file, "w", encoding="utf-8") as file_out:
            file_out.write(final_output)
        if debug:
            print(f"[DEBUG] Results saved to {output_file}")
    else:
        print(final_output)


def main() -> None:
    """
    Analyse les arguments pass√©s en ligne de commande et lance le scraping
    des d√©put√©s avec les options sp√©cifi√©es.

    Args:
        --list-regions (bool) : Affiche la liste des r√©gions valides et quitte.
        --region (str) : Liste des r√©gions √† scraper (ex: '√éle-de-France' 'Bretagne').
        --threads (int) : Nombre de threads √† utiliser (1 = ex√©cution s√©quentielle).
        --output (str) : Nom du fichier de sortie (si sp√©cifi√©).
        --debug (bool) : Active le mode debug pour plus de logs.
        --retries (int) : Nombre de tentatives en cas d'√©chec des requ√™tes.
        --delay (float) : D√©lai en secondes entre les tentatives.
        --timeout (float) : Timeout (secondes) des requ√™tes HTTP.
        --fields (str) : Champs √† r√©cup√©rer, s√©par√©s par une virgule (ex: "nom,email").
        --table (bool) : G√©n√®re un tableau ASCII des r√©sultats.
        --barefields (bool) : Affiche uniquement les valeurs sans labels.
        --no-separator (bool) : Supprime la ligne de s√©paration si --barefields + 1 champ.

    Returns:
        None: Ex√©cute le script et affiche les r√©sultats ou les enregistre.
    """
    parser = argparse.ArgumentParser(
        description="Scraping des informations des d√©put√©s fran√ßais (Nom, R√©gion, Email, Groupe, Circonscription) depuis le site officiel de l'Assembl√©e nationale."
    )

    # Option pour lister les r√©gions valides et quitter
    parser.add_argument("--list-regions", action="store_true",
                        help="Affiche la liste des r√©gions valides et quitte.")

    # R√©gions √† scraper
    parser.add_argument("--region", type=str, nargs="+",
                        help="R√©gions √† scraper (ex: 'Ile-de-France' 'Bretagne'). Par d√©faut, Ile-de-France et Provence-Alpes-C√¥te d'Azur.")

    # Options de scraping
    parser.add_argument("--threads", type=int, default=1,
                        help="Nombre de threads √† utiliser (1 = s√©quentiel).")
    parser.add_argument("--output", type=str,
                        help="Fichier o√π sauvegarder les r√©sultats.")
    parser.add_argument("--debug", action="store_true",
                        help="Active le mode debug pour afficher plus de logs.")
    parser.add_argument("--retries", type=int, default=3,
                        help="Nombre de tentatives en cas d'√©chec des requ√™tes (3 par d√©faut).")
    parser.add_argument("--delay", type=float, default=0.0,
                        help="D√©lai en secondes entre tentatives en cas d'√©chec (0 par d√©faut).")
    parser.add_argument("--timeout", type=float, default=10.0,
                        help="Timeout (secondes) des requ√™tes HTTP (10s par d√©faut).")
    parser.add_argument("--fields", type=str,
                        help="Champs √† r√©cup√©rer, s√©par√©s par virgules (ex: 'nom,email').")
    parser.add_argument("--table", action="store_true",
                        help="Affiche un tableau ASCII des r√©sultats.")
    parser.add_argument("--barefields", action="store_true",
                        help="Affiche uniquement les valeurs sans labels (ex: juste l'email).")
    parser.add_argument("--no-separator", action="store_true",
                        help="Si --barefields + 1 champ, supprime la ligne de s√©paration.")

    args = parser.parse_args()

    # Affichage de la liste des r√©gions valides
    if args.list_regions:
        print(f"üåç R√©gions valides :\n  - " + "\n  - ".join(VALID_REGIONS))
        exit(0)

    # V√©rification du nombre de threads
    if args.threads < 1:
        print("[ERROR] Le nombre de threads doit √™tre au moins 1.")
        exit(1)

    use_threads: bool = (args.threads > 1)

    # V√©rification et normalisation des r√©gions
    if args.region:
        selected_regions = [normalize_region(r) for r in args.region if normalize_region(r)]
        invalid_regions = [r for r in args.region if not normalize_region(r)]

        if invalid_regions:
            print(f"[ERROR] R√©gions invalides d√©tect√©es: {', '.join(invalid_regions)}")
            print(f"[INFO] Liste des r√©gions valides: {', '.join(VALID_REGIONS)}")
            exit(1)
    else:
        # Valeur par d√©faut si l'utilisateur ne sp√©cifie rien
        selected_regions = ["Ile-de-France", "Provence-Alpes-C√¥te d'Azur"]

    # Parse des fields (si --fields est sp√©cifi√©)
    if args.fields:
        selected_fields: List[str] = [f.strip() for f in args.fields.split(",")]
    else:
        selected_fields = None

    # Lancer le scraping
    scrape_deputes(
        regions=selected_regions,
        multithreading=use_threads,
        max_threads=args.threads,
        output_file=args.output,
        debug=args.debug,
        retries=args.retries,
        delay=args.delay,
        req_timeout=args.timeout,
        fields=selected_fields,
        use_table=args.table,
        barefields=args.barefields,
        no_separator=args.no_separator
    )


if __name__ == "__main__":
    main()
